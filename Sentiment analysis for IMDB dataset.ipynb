{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTjAns0CVpiA",
        "colab_type": "text"
      },
      "source": [
        "# **Sentiment Analysis for major airlines in United States**  [2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xojuT_YV3OEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eCzsqzZjJQH",
        "colab_type": "text"
      },
      "source": [
        "#**Dataset** [3]\n",
        "\n",
        "This dataset is of 14.5k tweets for each major airline in United States. The dataset consist of the following column classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"bad service\" or \"Can't Tell\").\n",
        "This dataset can be used for sentimental analysis as well as in the field of Data Science.\n",
        "\n",
        "I planned that I will use dataset for sentiment analysis and classify is the tweet a positive tweet or a negative tweet.\n",
        "\n",
        "I think so there are many other datasets like IMDb movie review, Rotten Tamato revies, Yelp dataset  that we can use for classification and they are accessible on internet. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un3wJi4l3Qbg",
        "colab_type": "code",
        "outputId": "adf0f031-e25e-47fb-8643-220f339fcbd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data/Tweets.csv')\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...               user_timezone\n",
              "0  570306133677760513  ...  Eastern Time (US & Canada)\n",
              "1  570301130888122368  ...  Pacific Time (US & Canada)\n",
              "2  570301083672813571  ...  Central Time (US & Canada)\n",
              "3  570301031407624196  ...  Pacific Time (US & Canada)\n",
              "4  570300817074462722  ...  Pacific Time (US & Canada)\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqhk6CdS3TAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews = np.array(data['text'])[:14000]\n",
        "labels = np.array(data['airline_sentiment'])[:14000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyGcp9sV3b7S",
        "colab_type": "code",
        "outputId": "6700fac2-bb0c-4992-cc9f-024abda9a64f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data['text'].loc[101]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'@VirginAmerica why must a traveler miss a flight to Late Flight check a bag?  I missed my morning appointments and you lost my business. #sfo2lax'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mvg-wlo3eFl",
        "colab_type": "code",
        "outputId": "8f5673fc-6b9b-438d-a8d6-f6fe877eb80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data['airline_sentiment'].loc[101]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP5FlMfRo9lv",
        "colab_type": "text"
      },
      "source": [
        "**Text Preprocessing**\n",
        "\n",
        "Text cleaning and preprocessing one of the major methods that need to carry out for NLP. The text that we use is a mess, they consist of slang,  its unstructured, grammar and syntax rule, which machine cannot understand. \n",
        "In order to machine to understand we preprocess the text data so that it is predictable and analyzable for machine. The preprocessing of the text data consist of following: \n",
        "*   Tokenization\n",
        "*   Removing Stop Words\n",
        "*   Lowercase the whole text data\n",
        "*   Word Embedding\n",
        "*   Removing the numbers\n",
        "*   Removing punctuations\n",
        "*   Removing white space\n",
        "*   Padding the sentence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-whwBnF3gnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "punctuation = '!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'\n",
        "\n",
        "# get rid of punctuation\n",
        "reviews = 'separator'.join(reviews)\n",
        "reviews = reviews.lower()\n",
        "text = ''.join([c for c in reviews if c not in punctuation])\n",
        "\n",
        "reviews_split = text.split('separator')\n",
        "text = ' '.join(reviews_split)\n",
        "\n",
        "words = text.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIt8dOAE3lhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_reviews = []\n",
        "for review in reviews_split:\n",
        "    review = review.split()\n",
        "    new_text = []\n",
        "    for word in review:\n",
        "        if (word[0] != '@') & ('http' not in word) & (~word.isdigit()):\n",
        "            new_text.append(word)\n",
        "    new_reviews.append(new_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4adFBp_3n75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
        "\n",
        "reviews_ints = []\n",
        "for review in new_reviews:\n",
        "    reviews_ints.append([vocab_to_int[word] for word in review])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fAlqyj43tSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_labels = []\n",
        "for label in labels:\n",
        "    if label == 'neutral':\n",
        "        encoded_labels.append(1)\n",
        "    elif label == 'negative':\n",
        "        encoded_labels.append(0)\n",
        "    else:\n",
        "        encoded_labels.append(1)\n",
        "\n",
        "encoded_labels = np.asarray(encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1RFdZtt3wF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def padding(reviews_ints, seq_length):\n",
        "  # getting the correct rows x cols shape\n",
        "  features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "\n",
        "  # for each review, I grab that review and \n",
        "  for i, row in enumerate(reviews_ints):\n",
        "      features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "  \n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5OYnJHP3zBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 30\n",
        "features = padding(reviews_ints, seq_length=seq_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-mLbXs131LB",
        "colab_type": "code",
        "outputId": "847fa9ac-210c-4373-b9ad-97c6455fcf3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "split = 0.8\n",
        "\n",
        "split_idx = int(len(features)*split)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of the resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: {}\".format(train_x.shape), \n",
        "      \"\\nValidation set:{}\".format(val_x.shape),\n",
        "      \"\\nTest set: {}\".format(test_x.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: (11200, 30) \n",
            "Validation set:(1400, 30) \n",
            "Test set: (1400, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp_NBULm35SR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmXQu2_B38U2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_iteration = iter(train_loader)\n",
        "sample_x, sample_y = data_iteration.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ0i9vOI3_Im",
        "colab_type": "code",
        "outputId": "a92f3055-b3ea-458a-f1e7-51baabf6d34a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device=torch.cuda.is_available()\n",
        "\n",
        "if(device):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zZa7KlyGvWX",
        "colab_type": "text"
      },
      "source": [
        "# Q1) Why did you choose this architecture?\n",
        "\n",
        "\n",
        "\n",
        "Ans: Reason behid choosing RNN-LSTM model is the data that I have choosen is made of sequence of words and RNN is known for short term dependencies, it also have feedback mechanism i.e. the current word can be dependent on previous word(s). While CNN do not have the feedback mechanism like RNN and CNN can be used only for extracting feature from dataset i.e. it can be better than RNN for extracting the meaning of the single word rather than the whole sentence. Even the RNN is known for short term dependencies it can store a small sentence, but if the dataset have a sentence of 30 words (I just aassumed that dataset may have the sentence with 30 words in it) RNN may lose the track of previous words. Hence I used LSTM so that it can store the whole sentence of 30 words.\n",
        "\n",
        "# Q2) What else did you try?\n",
        "\n",
        "Ans: I tried the RNN-GRU architecture becase GRU can be train faster than LSTM only on less data, they are simple and easy to modify. But if the data is huge and have long sequences then LSTM is better choice. In my case I had moderate amount of data (15 k tweets) on which GRU can work efficently. But there were some tweets that had a long sequence so I chose LSTM over GRU. Also I have worked on GRU last semeter for my CS440 class where I trained a chatbot using RNN-GRU netowrk and I wanted to try something else so this one was also one of the reason choosing LSTM over GRU. \n",
        "\n",
        "**I also tried using transformer but I failed to do so because its easy to understand but difficult to train. So I might give another try for tranformer in future.**\n",
        "\n",
        "# Q3) Explain your loss function. Did it perform as well as you had hoped?\n",
        "\n",
        "Ans: For loss function I used Binary Cross Entropy (aka log loss) because it is an binary classification problem. My LSTM model is returning the sigmoid value which is in the range 0 to 1. No, the loss function is performing quite bad because it's not linear.\n",
        "\n",
        "# Q4)How could you improve your model?\n",
        "\n",
        "Ans: I can improve my model using more dataset. The another approach is Transformer which I tried but was not able to build the network and I am thinking of giving it another try later in the semester."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEGFwpUV4B4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, output_size, embedding_dimension, hidden_dimension, n_layers, drop_prob=0.3):\n",
        "\n",
        "    super(RNN, self).__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dimension = hidden_dimension\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dimension)\n",
        "    self.lstm = nn.LSTM(embedding_dimension, hidden_dimension, n_layers, \n",
        "                        dropout=drop_prob, batch_first=True)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.fc = nn.Linear(hidden_dimension, output_size)\n",
        "    self.sig = nn.Sigmoid()\n",
        "      \n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    x = x.long()\n",
        "    embeds = self.embedding(x)\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    lstm_out = lstm_out.contiguous().view(-1, self.hidden_dimension)\n",
        "\n",
        "    out = self.dropout(lstm_out)\n",
        "    out = self.fc(out)\n",
        "    sig_out = self.sig(out)\n",
        "    sig_out = sig_out.view(batch_size, -1)\n",
        "    sig_out = sig_out[:, -1] # get last batch of labels\n",
        "    \n",
        "    # return last sigmoid output and hidden state\n",
        "    return sig_out, hidden\n",
        "  \n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    \n",
        "    if (device):\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dimension).zero_().cuda(),\n",
        "              weight.new(self.n_layers, batch_size, self.hidden_dimension).zero_().cuda())\n",
        "    else:\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dimension).zero_(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dimension).zero_())\n",
        "    \n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRom21_s4gW7",
        "colab_type": "code",
        "outputId": "9e12e53d-bd76-4bd4-d019-1a5d896f8efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dimension = 200\n",
        "hidden_dimension = 128\n",
        "n_layers = 2\n",
        "\n",
        "model = RNN(vocab_size, output_size, embedding_dimension, hidden_dimension, n_layers)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (embedding): Embedding(16728, 200)\n",
            "  (lstm): LSTM(200, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3eKB76H46io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr=0.01\n",
        "\n",
        "model_loss = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T52-K2Q4800",
        "colab_type": "code",
        "outputId": "5903e123-9acd-484d-924a-f0ab94ca0b8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "epochs = 10\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "gradient_clip=5 \n",
        "\n",
        "if(device):\n",
        "    model.cuda() # move model to GPU, if available\n",
        "\n",
        "model.train()\n",
        "\n",
        "for e in range(epochs):\n",
        "  hidden_state = model.init_hidden(batch_size)     # initialize hidden state\n",
        "\n",
        "  for inputs, labels in train_loader:\n",
        "      counter += 1\n",
        "      if(device):\n",
        "          inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      hidden_state = tuple([each.data for each in hidden_state])\n",
        "\n",
        "      model.zero_grad()\n",
        "      output, hidden_state = model(inputs, hidden_state)\n",
        "      loss = model_loss(output.squeeze(), labels.float())\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
        "      optimizer.step()\n",
        "\n",
        "      if counter % print_every == 0:\n",
        "          \n",
        "          val_hidden_state = model.init_hidden(batch_size) # Get validation loss\n",
        "          val_losses = []\n",
        "          model.eval()\n",
        "          for inputs, labels in valid_loader:\n",
        "\n",
        "              val_hidden_state = tuple([each.data for each in val_hidden_state])\n",
        "\n",
        "              if(device):\n",
        "                  inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "              output, val_hidden_state = model(inputs, val_hidden_state)\n",
        "              val_loss = model_loss(output.squeeze(), labels.float())\n",
        "\n",
        "              val_losses.append(val_loss.item())\n",
        "          \n",
        "          print(\"Epoch: {}/{}:\".format(e+1, epochs),\n",
        "                \"\\t Loss: {:.5f}:\".format(loss.item()))\n",
        "# plt.plot(val_losses)\n",
        "# plt.show() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10: \t Loss: 0.41127:\n",
            "Epoch: 1/10: \t Loss: 0.36212:\n",
            "Epoch: 2/10: \t Loss: 0.13671:\n",
            "Epoch: 2/10: \t Loss: 0.16968:\n",
            "Epoch: 3/10: \t Loss: 0.10537:\n",
            "Epoch: 3/10: \t Loss: 0.31508:\n",
            "Epoch: 4/10: \t Loss: 0.05865:\n",
            "Epoch: 4/10: \t Loss: 0.14510:\n",
            "Epoch: 5/10: \t Loss: 0.12971:\n",
            "Epoch: 5/10: \t Loss: 0.02323:\n",
            "Epoch: 5/10: \t Loss: 0.19623:\n",
            "Epoch: 6/10: \t Loss: 0.05141:\n",
            "Epoch: 6/10: \t Loss: 0.16443:\n",
            "Epoch: 7/10: \t Loss: 0.01325:\n",
            "Epoch: 7/10: \t Loss: 0.02685:\n",
            "Epoch: 8/10: \t Loss: 0.02043:\n",
            "Epoch: 8/10: \t Loss: 0.04405:\n",
            "Epoch: 9/10: \t Loss: 0.00413:\n",
            "Epoch: 9/10: \t Loss: 0.05652:\n",
            "Epoch: 9/10: \t Loss: 0.08037:\n",
            "Epoch: 10/10: \t Loss: 0.01336:\n",
            "Epoch: 10/10: \t Loss: 0.03484:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQGxC74_5A-d",
        "colab_type": "code",
        "outputId": "37422b36-b9fa-4d5e-bccc-b9efe345f4d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "hidden_state = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "  hidden_state = tuple([each.data for each in hidden_state])\n",
        "\n",
        "  if(device):\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "  \n",
        "  output, hidden_state = model(inputs, hidden_state)\n",
        "  \n",
        "  test_loss = model_loss(output.squeeze(), labels.float())\n",
        "  test_losses.append(test_loss.item())\n",
        "  \n",
        "  pred = torch.round(output.squeeze())  \n",
        "  correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "  correct = np.squeeze(correct_tensor.numpy()) if not device else np.squeeze(correct_tensor.cpu().numpy())\n",
        "  num_correct += np.sum(correct)\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))\n",
        "# plt.plot(test_losses)\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.790\n",
            "Test accuracy: 0.820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-KWmKDSsUOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_review(test_review):\n",
        "    test_review = test_review.lower() # lowercase\n",
        "    # get rid of punctuation\n",
        "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "\n",
        "    # splitting by spaces\n",
        "    test_words = test_text.split()\n",
        "    \n",
        "    # get rid of web address, twitter id, and digit\n",
        "    new_text = []\n",
        "    for word in test_words:\n",
        "        if (word[0] != '@') & ('http' not in word) & (~word.isdigit()):\n",
        "            new_text.append(word)\n",
        "\n",
        "    # tokens\n",
        "    test_ints = []\n",
        "    test_ints.append([vocab_to_int[word] for word in new_text])\n",
        "\n",
        "    return test_ints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVz7LMgh68Lr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, test_review, sequence_length=30):\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    test_ints = tokenize_review(test_review)\n",
        "    seq_length=sequence_length\n",
        "    features = padding(test_ints, seq_length)\n",
        "\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "\n",
        "    hidden_state = model.init_hidden(batch_size)\n",
        "    \n",
        "    if(device):\n",
        "        feature_tensor = feature_tensor.cuda()\n",
        "    output, hidden_state = model(feature_tensor, hidden_state)\n",
        "\n",
        "    pred = torch.round(output.squeeze()) \n",
        "    print('Prediction value: {:.12f}'.format(output.item()))\n",
        "    \n",
        "    # print custom response\n",
        "    if(pred.item()==1):\n",
        "        print(\"Positive review.\")\n",
        "    else:\n",
        "        print(\"Negative review.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuRwaHz47BR4",
        "colab_type": "code",
        "outputId": "916f3733-f6e4-48a9-8c0a-8a72947999a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "seq_length = 30\n",
        "test_review = \"@AmericanAir you have my money, you change my flight, and don't answer your phones! Any other suggestions so I can make my commitment??\"\n",
        "predict(model, test_review, seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction value: 0.045285705477\n",
            "Negative review.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hp9PmtS9PCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_text = \"@VirginAmerica I love the hipster innovation. You are a feel good brand.\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC5u9cPTB8pA",
        "colab_type": "code",
        "outputId": "37b53dc2-d66d-4145-dcc8-071d3093a208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "predict(model, test_text, seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction value: 0.999496698380\n",
            "Positive review.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C19R36LCAct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_review2 = \"@united  Delay DEN-CLE because they have to manually enter baggage tags? Really? Worst cust service day for this 1ker. #friendlyskies??\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kSpF-rrC7I7",
        "colab_type": "code",
        "outputId": "d2f3d6f3-d949-4b58-cd13-6fa722ea9c9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "predict(model, test_review2, seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction value: 0.000030525101\n",
            "Negative review.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5LEsx-rC-rY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_review3 = \"@United an idea: monitor mileage members travel patterns enough to know when they change jobs/lose status...and make the transition easier.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0ILZWl4DcW6",
        "colab_type": "code",
        "outputId": "8cafdb82-5288-42b1-a816-db338a4c88a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "predict(model,test_review3,seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction value: 0.999949097633\n",
            "Positive review.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhguETtdsna6",
        "colab_type": "text"
      },
      "source": [
        "# **Working on my data**\n",
        "My data consist of 100 positive and 100 negative teewts and it have an accuracy of 80%.\n",
        "\n",
        "**Model Strength And Weakness**\n",
        "\n",
        "Strength of my model is it have good accuracy and it is predicting the sentiments quite well.\n",
        "Weakness that I found is that loss is not linear i.e. it is very fluctuating. Also the dataset I worked on was not quite big so if there is word out of the vocabulary then it is throwing an error. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exihsilJDhG5",
        "colab_type": "code",
        "outputId": "5b62763c-a432-4463-b81a-f323b76a3c29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# import torch\n",
        "# model = torch.load('/content/drive/My Drive/Colab Notebooks/model')\n",
        "# model.eval()\n",
        "\"\"\"I have preprocessed my own data during preprocessing the orignal data\"\"\"\n",
        "valid_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "hidden_state = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in valid_loader:\n",
        "  hidden_state = tuple([each.data for each in hidden_state])\n",
        "\n",
        "  if(device):\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "  \n",
        "  output, hidden_state = model(inputs, hidden_state)\n",
        "  \n",
        "  test_loss = model_loss(output.squeeze(), labels.float())\n",
        "  test_losses.append(test_loss.item())\n",
        "  \n",
        "  pred = torch.round(output.squeeze())  \n",
        "  correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "  correct = np.squeeze(correct_tensor.numpy()) if not device else np.squeeze(correct_tensor.cpu().numpy())\n",
        "  num_correct += np.sum(correct)\n",
        "\n",
        "print(\"My data loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"My data accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My data loss: 0.852\n",
            "My data accuracy: 0.802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6IFpXdbpmx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/My Drive/Colab Notebooks/model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj7j7WGNRh45",
        "colab_type": "text"
      },
      "source": [
        "Reference:\n",
        "\n",
        "[1] https://towardsdatascience.com/sentiment-analysis-on-us-twitter-airline-dataset-1-of-2-2417f204b971\n",
        "\n",
        "[2] https://colab.research.google.com/github/agungsantoso/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Exercise.ipynb\n",
        "\n",
        "[3] Dataset Reference: https://www.kaggle.com/crowdflower/twitter-airline-sentiment#Tweets.csv"
      ]
    }
  ]
}